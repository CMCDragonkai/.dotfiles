#!/usr/bin/env sh

# gets a random user agents from leading crawlers
# use with care!

user_agents=(
    'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'
    'Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'
    'Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)'
    'Mozilla/5.0 (compatible; YandexBot/3.0; +http://yandex.com/bots)'
    'Mozilla/5.0 (compatible; Yahoo! Slurp; http://help.yahoo.com/help/us/ysearch/slurp)'
    'Mozilla/5.0 (compatible; Exabot/3.0; +http://www.exabot.com/go/robot)'
    'ia_archiver (+http://www.alexa.com/site/help/webmasters; crawler@alexa.com)'
    'DuckDuckBot/1.0; (+http://duckduckgo.com/duckduckbot.html)'
    'LinkedInBot/1.0 (compatible; Mozilla/5.0; Jakarta Commons-HttpClient/3.1 +http://www.linkedin.com)'
    'facebookexternalhit/1.1 (+http://www.facebook.com/externalhit_uatext.php)'
    'Pinterest/0.2 (+http://www.pinterest.com/bot.html)'
    'Mediumbot-MetaTagFetcher/0.3 (+https://medium.com/)'
    'Mozilla/5.0 (compatible; CloudFlare-AlwaysOnline/1.0; +http://www.cloudflare.com/always-online)'
    'Twitterbot/1.0'
)

length="${#user_agents[@]}"
lentgh="$((length - 1))"

index="$(shuf --input-range=1-"$length" --head-count=1)"

printf "${user_agents[$index]}"
